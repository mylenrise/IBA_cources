{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Describe how the AdaBoost algorithm works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost combines several learning algorithms. Firsty it trains some of its classification algorithms getting weights and prediction error. At each iteration of the training process, a weight is assigned to each sample in the training set equal to the current error on that sample. Updated weights are used for next next classifier and so on. To make a prediction, AdaBoost counts all the predictions in its classifiers using final weights. Predicted class is selected by the majority of weighted votes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the principal difference between AdaBoost and Gradient Boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting fits every next classifier to the residual error made by previous classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If your Gradient Boosting ensemble overfits the training set, should\n",
    "you increase or decrease the learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You should decrease the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Try to apply any boosting model you have already known to a real dataset. Choose simple vanilla accuracy for classification or mean square error for regression as your control metric. \n",
    "- Choose the best algorithm using cross validation and hyperparameter tuning.\n",
    "- Select the most important features. Did you expect to see any of those?\n",
    "- Try to analyse results and interpret them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer is in another notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*5. What are the main disadvantages of boosting algorithms? How to work around them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing can't be parallelized because each predictor can only be trained after the previous one. As the algorithms complexity is rather high, it takes a long time to apply. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*6. Does the Decision Tree algorithm are the single type of base estimator? Provide examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, there can be RandomForest or ExtraTree or any algorithm supporting for sample weighting and having proper classes_ and n_classes_ attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*7. If your AdaBoost ensemble underfits the training data, which\n",
    "hyperparameters should you tweak and how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase n_estimators and learning_rate and maybe tweak them in inner classificator. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
